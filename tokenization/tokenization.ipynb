{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e0011a-93fa-4b6b-b6c5-e505d7900894",
   "metadata": {},
   "source": [
    "## what is tokenization?\n",
    "\n",
    "it's the process of breaking text into smaller units (tokens) which are the input to the LLM. these tokens can be characters, subwords, words and byte sequences. the way text is tokenized directly affects model performance, generalization, vocabulary size and handling of rare/unknown words.\n",
    "\n",
    "we can't use whole words as tokens for a few reasons:\n",
    "\n",
    "1. the vocabulary quickly becomes very large.\n",
    "2. models struggle with rare or unseen words, such as typos or new names.\n",
    "3. morphologically rich languages (like german) create many variants of the same root word.\n",
    "\n",
    "this is why we implement **subword tokenization**, splitting words into subwords to reduce total vocabulary size but retain the complex relations in the text.\n",
    "\n",
    "- since the corpus that trains the tokenizer is almost entirely composed of english, the tokenization of other languages usually result in a higher vocab size, because the \"chunks\" are more broken up and we use a lot more tokens for the *exact same thing*, bloating the sequence length of the documents and overflowing the maximum context length of attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2bbdc-5d7e-4d9e-8c6f-e19fd11e6e8b",
   "metadata": {},
   "source": [
    "## byte-pair encoding\n",
    "is a popular subowrd tokenization approach. the most common pair of bytes in a corpus are iteratively combined until the necessary vocabulary size is attained, creating a collection of subwords. \n",
    "\n",
    "1. start with a base vocabulary of characters (ex. 'a','b','c',...).\n",
    "2. represent each word as a sequence of characters + end-of-word symbol.\n",
    "3. count the most frequent pair of symbols in the training data.\n",
    "4. merge the most frequent pair into a new symbol and update all occurrences.\n",
    "5. repeat 3-4 for a fixed number of merges or until the vocabulary reaches a certain size.\n",
    "\n",
    "the main advantages of BPE is how well it adapts to rare words (by breaking them into known subwords) and how it limits vocabulary size. also, it remains robust across many languages.\n",
    "\n",
    "however, BPE is frequency-based, not linguistically motivated, so some splits may break grammar relations. also, merges are fixed, so the tokenizer doesn't adapt to new domains, and it may treat semantically similar words differently if they tokenize differently.\n",
    "\n",
    "**example:** considering the sequence ```aaabdaaabac```:\n",
    "\n",
    "1. the byte pair ``aa`` appears twice, so we make it a new byte pair ``Z``: ```ZabdZabac```\n",
    "2. the byte pair ``ab`` appears twice, so we make it a new byte pair ``Y``: ``ZYbZYac``\n",
    "3. the byte pair ``ZY`` appears twice, so we can make a new byte pair ``X``: ``XbXac``\n",
    "4. there are no more byte pairs that appear more than once, so we're done.\n",
    "\n",
    "to de-tokenize the data, we simply perform the replacements in the reverse order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db530707-2728-43f5-999e-d46d165ad7cc",
   "metadata": {},
   "source": [
    "## embedding table\n",
    "after tokenization, our corpus becomes a sequence of token IDs, like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03c917-d817-4068-abd0-034fd6580784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"unbelievable\" -> [\"un\",\"believ\",\"able\"] -> [152,6201,398]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a3ee1-8cb6-4c7b-9eca-b9a35a008f83",
   "metadata": {},
   "source": [
    "but neural networks don't operate on IDs, they need numerical vectors, so each token ID is **mapped to a dense vector** using the embedding table. \n",
    "\n",
    "the embedding table is a ```vocab_size X embedding_dim``` matrix where each row corresponds to a token. when the model sees a token ID ```i```, it looks up row i to get the token's embedding. this is the vector that is fed into the transformer. \n",
    "\n",
    "it is **learned during training**, captures semantic meaning (similar words or subwords have similar vectors) and it allows the model to generalize across words or subwords with similar usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719c2e9-5186-44ac-a2f0-756e83087218",
   "metadata": {},
   "source": [
    "## coding our tokenizer\n",
    "first, we need to understand that python uses *unicode* for encoding each character. specifically, we usually use UTF-8 to take unicode text and transform it into binary strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19aa424-a77c-4c39-a671-36dcf073c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 128075 24859\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"h\"), ord(\"üëã\"), ord(\"ÊÑõ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05477663-fbcd-473b-8c6a-31c614f43d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "list((\"„Åì„Çì„Å´„Å°„ÅØ üëã ol√°!\").encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93787cd7-be3d-4076-93b6-5d4b26f779e0",
   "metadata": {},
   "source": [
    "if we just used UTF-8 as our tokens, we would have a vocabulary length of 256 possible tokens, which is very small, and would make our text streched out over very long sequences of bytes, messing up our context length. \n",
    "\n",
    "since we don't want to use our raw bytes, we turn to the BPE algorithm to compress the byte sequences. for this example, i chose the first paragraph of tolstoy's anna karenina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29744584-3a86-449d-beeb-af04480c9a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1163\n",
      "token length: 1163\n"
     ]
    }
   ],
   "source": [
    "text = \"Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys' house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Every person in the house felt that there was so sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the members of the family and household of the Oblonskys. The wife did not leave her own room, the husband had not been at home for three days. The children ran wild all over the house; the English governess quarreled with the housekeeper, and wrote to a friend asking her to look out for a new situation for her; the man-cook had walked off the day before just at dinner time; the kitchen-maid, and the coachman had given warning.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "print(f\"text length: {len(text)}\\ntoken length: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f99d1f2c-58ac-4de4-8547-0d468c72bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(ids):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ids: a list of integers.\n",
    "    returns:\n",
    "    - counts: a dictionary whose key is the pair and value is the amount of time they appeared.\n",
    "    iterates on every consecutive element in the ids vector and adds one to counts for every pair found\n",
    "    '''\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # iterating consecutive elements\n",
    "        counts[pair] = counts.get(pair,0) + 1 # incrementing for each pair found\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a93316-d62b-4660-b046-0c36a0cc56fd",
   "metadata": {},
   "source": [
    "now, we can see how often each pair appears when applying the tokenized text into this function. the pair most often found was ```(101, 32)```, which represents the characters ```'e'``` and ```' '```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe4093-e323-45a1-a90b-901cdfda28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = find_pairs(tokens)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse = True)) # v = # found, k = pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e978b6-3349-4c17-b460-7674da690cd9",
   "metadata": {},
   "source": [
    "we have the pairs, so we'll iterate over this entire list and start minting the paired bytes, starting for the most common. since we're working with UTF-8 encoding, which has a maximum value of 255, we will start our new tokens at 256, and grow from there. so the new token for the pair ```(101,32)``` will be ```(256)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c20a966-0bab-443e-be85-771a62361942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ids: a list of integers\n",
    "    - pair: the pair we want to replace\n",
    "    - idx: the token we will replace them with\n",
    "    returns:\n",
    "    - new_ids: a list of integers with every instance of pair changed for idx\n",
    "    replaces every instance of the pair with idx\n",
    "    '''\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59a02250-1b4d-4147-b1cc-b48e1dd21b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  1119\n"
     ]
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(\"length: \", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b3481-0fa3-4759-9e13-d371d4702fff",
   "metadata": {},
   "source": [
    "which makes total sense, because our original tokenized length was 1163 and we had 44 occurences of the pair ```(101, 32)```, so now our length is 1163 - 44 = 1119.\n",
    "\n",
    "now we just iterate this function. how much we iterate is a *hyperparameter*: the more steps we take, the larger will be our vocabulary and the shorter will be our sequences. we need to find the sweet spot that works the best in practice.\n",
    "\n",
    "- for example, GPT-4 uses around 50.000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf879f1c-a9ee-4033-986f-f2c40d62dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = find_pairs(ids)\n",
    "    top_pair = max(stats,key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging {top_pair} into {idx}\")\n",
    "    ids = merge(ids, top_pair, idx)\n",
    "    merges[top_pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "347922b2-8ff9-4cdc-b78c-473f0452c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 1163\n",
      "compressed length: 821\n",
      "compression ratio: 1.42x\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokens length: {len(tokens)}\\ncompressed length: {len(ids)}\\ncompression ratio: {len(tokens) / len(ids):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532c97a-213c-4a55-8889-66f09f7e9c15",
   "metadata": {},
   "source": [
    "the choice of the number of merges, as discussed, is a hyperparameter. in this example i chose to do exactly 20 merges, which was enough to compress the text by 1.42x!\n",
    "\n",
    "the merges dictionary works as an inverse tree, where we start with the leaves and build up the new tokens from them. it's necessary for us to see what changes the tokenizer did. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28370c02-0dd0-4959-9a6a-c07a64a82161",
   "metadata": {},
   "source": [
    "## but where actually is the tokenizer?\n",
    "the tokenizer is a **completely separate, independent module** from the LLM. it has it's own training set of text (which is can be different from that of the LLM). it then translates back and forth between raw text and sequences of tokens. the LLM only ever sees the tokens and *never* directly deals with any text.\n",
    "\n",
    "once our tokenizer is trained (it has both the vocabulary and the merges), we can do the encoding and decoding. \n",
    "\n",
    "## decoding\n",
    "given a sequence of integers in the range ```[ 0, vocab_size]```, how can we get a string object?\n",
    "\n",
    "1. first up, we build a ```vocab``` dictionary populated by all possible single-byte values, each mapped to its byte representation.\n",
    "2. then we add the merges we did, available on the merge dictionary. for example, if we merged the pair ``'a'`` and ``'b'``, we got ``'ab'``, it will be represented in ``vocab``.\n",
    "3. to find the real text, we concatenate all the token byte sequence together into one byte object and decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52f3e093-acb3-4582-949b-4800ef28ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range (256)}\n",
    "for (pair0, pair1), idx in merges.items():\n",
    "    vocab[idx] = vocab[pair0] + vocab[pair1]\n",
    "\n",
    "def decode(ids):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ids: the tokens of our texto\n",
    "    returns:\n",
    "    - text: the string decoded\n",
    "    '''\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids) # one way of concatenating bytes together\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
