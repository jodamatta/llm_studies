{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e0011a-93fa-4b6b-b6c5-e505d7900894",
   "metadata": {},
   "source": [
    "## what is tokenization?\n",
    "\n",
    "it's the process of breaking text into smaller units (tokens) which are the input to the LLM. these tokens can be characters, subwords, words and byte sequences. the way text is tokenized directly affects model performance, generalization, vocabulary size and handling of rare/unknown words.\n",
    "\n",
    "we can't use whole words as tokens for a few reasons:\n",
    "\n",
    "1. the vocabulary quickly becomes very large.\n",
    "2. models struggle with rare or unseen words, such as typos or new names.\n",
    "3. morphologically rich languages (like german) create many variants of the same root word.\n",
    "\n",
    "this is why we implement **subword tokenization**, splitting words into subwords to reduce total vocabulary size but retain the complex relations in the text.\n",
    "\n",
    "- since the corpus that trains the tokenizer is almost entirely composed of english, the tokenization of other languages usually result in a higher vocab size, because the \"chunks\" are more broken up and we use a lot more tokens for the *exact same thing*, bloating the sequence length of the documents and overflowing the maximum context length of attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2bbdc-5d7e-4d9e-8c6f-e19fd11e6e8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## byte-pair encoding\n",
    "is a popular subowrd tokenization approach. the most common pair of bytes in a corpus are iteratively combined until the necessary vocabulary size is attained, creating a collection of subwords. \n",
    "\n",
    "1. start with a base vocabulary of characters (ex. 'a','b','c',...).\n",
    "2. represent each word as a sequence of characters + end-of-word symbol.\n",
    "3. count the most frequent pair of symbols in the training data.\n",
    "4. merge the most frequent pair into a new symbol and update all occurrences.\n",
    "5. repeat 3-4 for a fixed number of merges or until the vocabulary reaches a certain size.\n",
    "\n",
    "the main advantages of BPE is how well it adapts to rare words (by breaking them into known subwords) and how it limits vocabulary size. also, it remains robust across many languages.\n",
    "\n",
    "however, BPE is frequency-based, not linguistically motivated, so some splits may break grammar relations. also, merges are fixed, so the tokenizer doesn't adapt to new domains, and it may treat semantically similar words differently if they tokenize differently.\n",
    "\n",
    "**example:** considering the sequence ```aaabdaaabac```:\n",
    "\n",
    "1. the byte pair ``aa`` appears twice, so we make it a new byte pair ``Z``: ```ZabdZabac```\n",
    "2. the byte pair ``ab`` appears twice, so we make it a new byte pair ``Y``: ``ZYbZYac``\n",
    "3. the byte pair ``ZY`` appears twice, so we can make a new byte pair ``X``: ``XbXac``\n",
    "4. there are no more byte pairs that appear more than once, so we're done.\n",
    "\n",
    "to de-tokenize the data, we simply perform the replacements in the reverse order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db530707-2728-43f5-999e-d46d165ad7cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## embedding table\n",
    "after tokenization, our corpus becomes a sequence of token IDs, like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03c917-d817-4068-abd0-034fd6580784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"unbelievable\" -> [\"un\",\"believ\",\"able\"] -> [152,6201,398]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a3ee1-8cb6-4c7b-9eca-b9a35a008f83",
   "metadata": {},
   "source": [
    "but neural networks don't operate on IDs, they need numerical vectors, so each token ID is **mapped to a dense vector** using the embedding table. \n",
    "\n",
    "the embedding table is a ```vocab_size X embedding_dim``` matrix where each row corresponds to a token. when the model sees a token ID ```i```, it looks up row i to get the token's embedding. this is the vector that is fed into the transformer. \n",
    "\n",
    "it is **learned during training**, captures semantic meaning (similar words or subwords have similar vectors) and it allows the model to generalize across words or subwords with similar usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719c2e9-5186-44ac-a2f0-756e83087218",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## coding our tokenizer\n",
    "first, we need to understand that python uses *unicode* for encoding each character. specifically, we usually use UTF-8 to take unicode text and transform it into binary strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19aa424-a77c-4c39-a671-36dcf073c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 128075 24859\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"h\"), ord(\"üëã\"), ord(\"ÊÑõ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05477663-fbcd-473b-8c6a-31c614f43d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "list((\"„Åì„Çì„Å´„Å°„ÅØ üëã ol√°!\").encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93787cd7-be3d-4076-93b6-5d4b26f779e0",
   "metadata": {},
   "source": [
    "if we just used UTF-8 as our tokens, we would have a vocabulary length of 256 possible tokens, which is very small, and would make our text streched out over very long sequences of bytes, messing up our context length. \n",
    "\n",
    "since we don't want to use our raw bytes, we turn to the BPE algorithm to compress the byte sequences. for this example, i chose the first paragraph of tolstoy's anna karenina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29744584-3a86-449d-beeb-af04480c9a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1163\n",
      "token length: 1163\n"
     ]
    }
   ],
   "source": [
    "text = \"Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys' house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Every person in the house felt that there was so sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the members of the family and household of the Oblonskys. The wife did not leave her own room, the husband had not been at home for three days. The children ran wild all over the house; the English governess quarreled with the housekeeper, and wrote to a friend asking her to look out for a new situation for her; the man-cook had walked off the day before just at dinner time; the kitchen-maid, and the coachman had given warning.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "print(f\"text length: {len(text)}\\ntoken length: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99d1f2c-58ac-4de4-8547-0d468c72bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(ids):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ids: a list of integers.\n",
    "    returns:\n",
    "    - counts: a dictionary whose key is the pair and value is the amount of time they appeared.\n",
    "    iterates on every consecutive element in the ids vector and adds one to counts for every pair found\n",
    "    '''\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # iterating consecutive elements\n",
    "        counts[pair] = counts.get(pair,0) + 1 # incrementing for each pair found\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a93316-d62b-4660-b046-0c36a0cc56fd",
   "metadata": {},
   "source": [
    "now, we can see how often each pair appears when applying the tokenized text into this function. the pair most often found was ```(101, 32)```, which represents the characters ```'e'``` and ```' '```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe4093-e323-45a1-a90b-901cdfda28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = find_pairs(tokens)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse = True)) # v = # found, k = pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e978b6-3349-4c17-b460-7674da690cd9",
   "metadata": {},
   "source": [
    "we have the pairs, so we'll iterate over this entire list and start minting the paired bytes, starting for the most common. since we're working with UTF-8 encoding, which has a maximum value of 255, we will start our new tokens at 256, and grow from there. so the new token for the pair ```(101,32)``` will be ```(256)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c20a966-0bab-443e-be85-771a62361942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ids: a list of integers\n",
    "    - pair: the pair we want to replace\n",
    "    - idx: the token we will replace them with\n",
    "    returns:\n",
    "    - new_ids: a list of integers with every instance of pair changed for idx\n",
    "    replaces every instance of the pair with idx\n",
    "    '''\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a02250-1b4d-4147-b1cc-b48e1dd21b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  1119\n"
     ]
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(\"length: \", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b3481-0fa3-4759-9e13-d371d4702fff",
   "metadata": {},
   "source": [
    "which makes total sense, because our original tokenized length was 1163 and we had 44 occurences of the pair ```(101, 32)```, so now our length is 1163 - 44 = 1119.\n",
    "\n",
    "now we just iterate this function. how much we iterate is a *hyperparameter*: the more steps we take, the larger will be our vocabulary and the shorter will be our sequences. we need to find the sweet spot that works the best in practice.\n",
    "\n",
    "- for example, GPT-4 uses around 50.000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf879f1c-a9ee-4033-986f-f2c40d62dd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 32) into 256\n",
      "Merging (116, 104) into 257\n",
      "Merging (32, 257) into 258\n",
      "Merging (100, 32) into 259\n",
      "Merging (101, 114) into 260\n",
      "Merging (110, 32) into 261\n",
      "Merging (258, 256) into 262\n",
      "Merging (97, 110) into 263\n",
      "Merging (105, 110) into 264\n",
      "Merging (117, 115) into 265\n",
      "Merging (121, 32) into 266\n",
      "Merging (115, 32) into 267\n",
      "Merging (104, 111) into 268\n",
      "Merging (116, 32) into 269\n",
      "Merging (104, 97) into 270\n",
      "Merging (263, 259) into 271\n",
      "Merging (119, 105) into 272\n",
      "Merging (270, 259) into 273\n",
      "Merging (44, 32) into 274\n",
      "Merging (118, 260) into 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = find_pairs(ids)\n",
    "    top_pair = max(stats,key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging {top_pair} into {idx}\")\n",
    "    ids = merge(ids, top_pair, idx)\n",
    "    merges[top_pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347922b2-8ff9-4cdc-b78c-473f0452c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 1163\n",
      "compressed length: 821\n",
      "compression ratio: 1.42x\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokens length: {len(tokens)}\\ncompressed length: {len(ids)}\\ncompression ratio: {len(tokens) / len(ids):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532c97a-213c-4a55-8889-66f09f7e9c15",
   "metadata": {},
   "source": [
    "the choice of the number of merges, as discussed, is a hyperparameter. in this example i chose to do exactly 20 merges, which was enough to compress the text by 1.42x!\n",
    "\n",
    "the merges dictionary works as an inverse tree, where we start with the leaves and build up the new tokens from them. it's necessary for us to see what changes the tokenizer did. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28370c02-0dd0-4959-9a6a-c07a64a82161",
   "metadata": {},
   "source": [
    "## but where actually is the tokenizer?\n",
    "the tokenizer is a **completely separate, independent module** from the LLM. it has it's own training set of text (which is can be different from that of the LLM). it then translates back and forth between raw text and sequences of tokens. the LLM only ever sees the tokens and *never* directly deals with any text.\n",
    "\n",
    "once our tokenizer is trained (it has both the vocabulary and the merges), we can do the encoding and decoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d98070-e9ec-4102-9854-d566e106f3bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## decoding\n",
    "given a sequence of integers in the range ```[ 0, vocab_size]```, how can we get a string object?\n",
    "\n",
    "1. first up, we build a ```vocab``` dictionary populated by all possible single-byte values, each mapped to its byte representation.\n",
    "2. then we add the merges we did, available on the merge dictionary. for example, if we merged the pair ``'a'`` and ``'b'``, we got ``'ab'``, it will be represented in ``vocab``.\n",
    "3. to find the real text, we concatenate all the token byte sequence together into one byte object and decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f3e093-acb3-4582-949b-4800ef28ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range (256)}\n",
    "for (pair0, pair1), idx in merges.items():\n",
    "    vocab[idx] = vocab[pair0] + vocab[pair1]\n",
    "\n",
    "def decode(ids):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ids: the tokens of our text\n",
    "    returns:\n",
    "    - text: the string decoded\n",
    "    '''\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids) # one way of concatenating bytes together\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94626615-d07a-4050-b1c3-239c92a3d3c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## encoding\n",
    "given a string, what are the tokens?\n",
    "\n",
    "1. first, we encode the tokens into UTF-8\n",
    "2. but wait, some of the bytes may be merged, and some merges can be made over previous merges, so we need to respect the order that the merges where made.\n",
    "3. so we need to find the lowest-index merge, in this case 256. for every pair inside stats, we look at all of their index and we take the minimum value.\n",
    "4. having the pair, call the merge function on that pair, for idx.\n",
    "5. we do that until there are no more mergeable pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b16bc53-0c9c-4149-a6eb-ab491d699747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = find_pairs(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddec464b-3f2f-45f7-9b6b-43d038738a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110, 111, 32, 116, 114, 117, 99, 256, 272, 257, 262, 102, 117, 114, 105, 101, 115]\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"no truce with the furies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4204e3aa-9f66-4af4-b8df-0210acd40ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no truce with the furies\n"
     ]
    }
   ],
   "source": [
    "print(decode([110, 111, 32, 116, 114, 117, 99, 256, 272, 257, 262, 102, 117, 114, 105, 101, 115]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c7ef1-92ff-46d6-bb96-e53d250f32de",
   "metadata": {},
   "source": [
    "## regex patterns\n",
    "so we have a very basic tokenizer for now. yay! but how can we improve it? for example, for **GPT-2**, their tokenizer proposed not generating multiple tokens for the same word based on punctuation. for example, ```dog```, `dog!`, and `dog.` were all different tokens, which results in a sub-optimal allocation of already limited vocabulary slots and model capacity.\n",
    "\n",
    "- so to avoid this, they prevented the BPE algorithm top-down from merging across character categories for any byte squence. this means that some types of characters must never be merged together.\n",
    "- this is done via a regex pattern, which allows them to enforce these rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066f13c9-357f-4531-9764-7059a09736a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ',', ' right', ' now', ' it', \"'s\", ' 7', ':', '00', 'pm', '!!!']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pattern, \"Hello world, right now it's 7:00pm!!!\")) # take the patern and match it agains the string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e3753-4c98-4451-a175-ec59f57ab42a",
   "metadata": {},
   "source": [
    "with this pattern, we first splitting the text up according to our pattern, into a list of texts and all of these elements are processed independently by the tokenizer, and the result is simply concatenated.\n",
    "\n",
    "for this pattern specifically, we're avoiding merging words, punctuation, number and spaces into the same tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e43a3-fe13-41f8-aa51-e2f01de9ac87",
   "metadata": {},
   "source": [
    "## tiktoken\n",
    "**tiktoken** is the official open-ai library for tokenization inference. there's no training code, just the inference code. it can give us the gpt-2 and gpt-4 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d62efe-f6ba-4a7d-b919-600164e0e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 23748, 995, 0]\n",
      "[256, 24748, 1917, 0]\n",
      "[370, 582, 349, 665, 33050, 3209, 2172]\n",
      "[12158, 3919, 68346]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"   hello world!\"))\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") #gtp4 tokenizer\n",
    "print(enc.encode(\"   hello world!\"))\n",
    "\n",
    "print(enc.encode(\"abacate engra√ßado\"))\n",
    "print(enc.encode(\"funny avocado\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa2fd6-099e-4617-b269-001dd5bf3257",
   "metadata": {},
   "source": [
    "as you can see, the tokenization mechanism is different. one big difference is that the gpt2 tokenizer is hard-coded to not merge spaces, maybe because it was an advantage at the time for dealing with python identation, but the gpt4 tokenizer does merge them. \n",
    "\n",
    "also, in both cases, the words `hello` and `world` are single tokens, as is `ol√°` and `mundo`. \n",
    "\n",
    "however, less common words like `abacate` and `engra√ßado` take up way more tokens than `funny` and `avocado`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efede8a-1c20-40af-8262-4b393cb561d5",
   "metadata": {},
   "source": [
    "here's the **GPT-4** regex pattern. it has changed a bunch from the GPT-2 one. the first i causes case-insensitive match for the apostrophe examples, there's a lot more handling of the spaces, and the numbers are only merged up to three digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f551d73a-fac9-4d58-8f04-73727c828c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ',', ' right', ' now', ' it', \"'s\", ' ', '7', ':', '000', '0', 'pm', '!!!']\n"
     ]
    }
   ],
   "source": [
    "gpt4pattern = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s\"\"\")\n",
    "\n",
    "print(re.findall(gpt4pattern, \"Hello world, right now it's 7:0000pm!!!\")) # take the patern and match it agains the string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca006f18-dd69-41df-ac40-7f2c012afffe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## let's snoop around the GPT-2 tokenizer\n",
    "open-ai has made the gpt-2 tokenizer and their vocab file available for us to snoop, so that's exactly what we're doing. that's the closes they've been to *open*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8822fa38-d97e-481f-a1f8-0c98079c2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f)\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding = 'utf-8') as f:\n",
    "    bpe_data = f.read()\n",
    "\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c49920-7e51-446b-b412-8cf47738379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        '''\n",
    "        open-ai has included a byte encoder/decoder to be used serially with the encoder/decoder. this is more an architectural \n",
    "        choice than a theoretical one, so we won't go into much detail\n",
    "        '''\n",
    "        self.byte_encoder = bytes_to_unicode() \n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        '''\n",
    "        this is still the gpt-2 pattern, that doesn't like merging spaces and doesn't consider upper-case apostrophes.\n",
    "        '''\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        '''\n",
    "        avoid running into an error if the input cannot be paired.\n",
    "        '''\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        '''\n",
    "        this loop is very similar to what we've already covered.\n",
    "        '''\n",
    "        while True:\n",
    "            '''\n",
    "            we search for the top pair (this time called bigram)\n",
    "            '''\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            '''\n",
    "            they merge the top pair every time they find it in the token sequence.\n",
    "            '''\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    '''\n",
    "    the encode and decode function are pretty much the exact same that we've already coded\n",
    "    '''\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e346d-ad4e-4cc8-adb7-9c719f52970c",
   "metadata": {},
   "source": [
    "## special tokens\n",
    "we can use different tokens to delimit different parts of the data or to create a special structure of the token strings. open-ai did this with a `<|endoftext|>` token. they are mostly used in the fine-tuning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f78a26-218f-4196-9938-425fe888309d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder) # 256 raw byte tokens + 50.000 merges + 1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20b2920-ad74-444b-b6e7-dc946c8cd8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>'] # used to delimit documents in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993aa1a-b17c-4c21-a786-a41aa322062a",
   "metadata": {},
   "source": [
    "we can add as many tokens as we want extending the tiktoken library, creating our own vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "285ab377-0c5e-4f77-b656-e07c6a09c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "enc = tiktoken.Encoding(\n",
    "    name = \"cl100k_im\",\n",
    "    pat_str = cl100k_base._pat_str,\n",
    "    mergeable_ranks = cl100k_base._mergeable_ranks,\n",
    "    special_tokens = {\n",
    "        **clk100k_base._special_tokens, # include all of the usual special tokens\n",
    "        \"omg a new token\": 100264,      # and our new tokens!\n",
    "        \"omg hiiii\": 100265,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
